{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "In the notebook, I will take a look at the structure of data and clean as necessary. I'll also run preprocessing for text data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "judge-1377884607_tweet_product_company.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('DATA/judge-1377884607_tweet_product_company.csv', engine = 'python')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_text                                               1\n",
       "emotion_in_tweet_is_directed_at                       5802\n",
       "is_there_an_emotion_directed_at_a_brand_or_product       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot infer a tweet so I'll remove the missing tweet_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset = ['tweet_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether the tweet is towards apple or google is post-tweet variable, so it isn't relevant for this problem. So I'll entirely removed this columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('emotion_in_tweet_is_directed_at', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Columns\n",
    "I'll just make the columns a little bit easier to call out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['tweet', 'sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abnormal sentiment\n",
    "Check if there's any abnormal sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5388\n",
       "Positive emotion                      2978\n",
       "Negative emotion                       570\n",
       "I can't tell                           156\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No original tweet\n",
    "If the text only contains RT and not actual texts, we will be judging based on the previous tweet, so we should remove these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove RT text\n",
    "def remove_RT(str_):\n",
    "    return re.sub('(RT.+)', '', str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].map(remove_RT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['tweet'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    4133\n",
       "Positive emotion                      2406\n",
       "Negative emotion                       474\n",
       "I can't tell                           134\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split\n",
    "I'll divide the train/test split here so we can avoid any data leakage during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['tweet']\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll also separate validation set out of train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "X_train.to_csv('DATA/X_train.csv')\n",
    "y_train.to_csv('DATA/y_train.csv')\n",
    "X_test.to_csv('DATA/X_test.csv')\n",
    "y_test.to_csv('DATA/y_test.csv')\n",
    "X_val.to_csv('DATA/X_val.csv')\n",
    "y_val.to_csv('DATA/y_val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "I'll take a look at text data and turn it into a sustainable measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From randomly sampled tweets, I decided to do following: \n",
    "\n",
    "1. find any links and remove them (e.g. bit.ly/g03MZB)\n",
    "2. `@mention`, `{link}`\n",
    "Create a feature that shows how many of these exists, but remove them from tokenization.\n",
    "3. `[\\w+]` seems to be emoji or pics/videos. keep them as is (with brackets) \n",
    "3. word after # should be treated as a separate word than the same word without #, also count how many tags\n",
    "4. `?&quot;`, `$amp;`, `ï¿½` should be removed.\n",
    "5. ! and ? might be important to keep. count how many. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = X_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find links and turn them into {link}\n",
    "def replace_links(str_):\n",
    "    p = re.compile('((http\\w*:\\/\\/)?(www\\.\\w+)?(\\w+\\.(com|co|ly|ch|org|net)+)(\\/\\w+)?)')\n",
    "    return str_.replace(p, '{link}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many times @mention, {link} and [pic] occurs \n",
    "def count_exp(str_, exp):\n",
    "    p = re.compile(exp)\n",
    "    return len(p.findall(str_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove html symbol\n",
    "def remove_html(series_):\n",
    "    return series_.map(lambda x: re.sub(\"[^A-Za-z0-9 ]\\w+;\", '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary punctuations \n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "table_ = str.maketrans('', '', '!?#[]')\n",
    "punctuations = punctuations.translate(table_) + 'ï¿½'\n",
    "\n",
    "def remove_punctuations(str_, punctuations):\n",
    "    table_ = str.maketrans('', '', punctuations)\n",
    "    return str_.translate(table_)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# we don't want overall distribution of customer \n",
    "# satisfaction of each company to affect our analysis \n",
    "# so I'll remove some of the specific words\n",
    "\n",
    "stopwords += ['apple', 'android', 'ipad', 'iphone', 'sxsw', '#sxsw', 'intel', \n",
    "             'atari', 'cisco', 'google', 'genentech', 'mac', 'pc']\n",
    "\n",
    "stopwords += string.punctuation\n",
    "stopwords += string.digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# now tokenize and remove stopwords\n",
    "def tokenize(str_, stopwords):\n",
    "    str_ = str_.lower()\n",
    "    return [x for x in word_tokenize(str_) if x not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_text(series_, punctuations, stopwords):\n",
    "    series_ = replace_links(series_)\n",
    "    series_ = remove_html(series_)\n",
    "    series_ = series_.map(lambda x: remove_punctuations(x, punctuations))\n",
    "    df = pd.DataFrame(series_)\n",
    "    \n",
    "    df['mention_count'] = series_.map(lambda x: count_exp(x, '@\\w+'))\n",
    "    df['link_count'] = series_.map(lambda x: count_exp(x, '{link}')) \n",
    "    df['tag_count'] = series_.map(lambda x: count_exp(x, '#\\w+'))\n",
    "    df['exclam_count'] = series_.map(lambda x: count_exp(x, '!'))\n",
    "    df['quest_count'] = series_.map(lambda x: count_exp(x, '\\?'))\n",
    "    df['tweet'] = df.tweet.map(lambda x: tokenize(x, stopwords))\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
