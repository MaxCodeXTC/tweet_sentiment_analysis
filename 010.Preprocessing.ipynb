{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "I'll take a look at text data and turn it into quantifiable measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From randomly sampled tweets, I decided to do at least the following: \n",
    "\n",
    "1. find any links and remove them (e.g. bit.ly/g03MZB)\n",
    "2. `@mention`, `{link}`\n",
    "Create a feature that shows how many of these exists, but remove them from tokenization.\n",
    "3. `[\\w+]` seems to be emoji or pics/videos. keep them as is (with brackets) \n",
    "3. word after # should be treated as a separate word than the same word without #, also count how many tags\n",
    "4. `?&quot;`, `$amp;`, `�` should be removed.\n",
    "5. ! and ? might be important to keep. count how many. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('DATA/X_train.csv', index_col=0)\n",
    "X_val = pd.read_csv('DATA/X_val.csv', index_col=0)\n",
    "X_test = pd.read_csv('DATA/X_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find links and turn them into {link}\n",
    "def replace_links(str_):\n",
    "    p = '((http\\w*:\\/\\/)?(www\\.\\w+)?(\\w+\\.(com|co|ly|ch|org|net)+)(\\/\\w+)?)'\n",
    "    return re.sub(p, '{link}', str_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count mentions and links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many times @mention, and {link} occurs \n",
    "def count_exp(str_, exp):\n",
    "    p = re.compile(exp)\n",
    "    return len(p.findall(str_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove HTML symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove html symbol\n",
    "def remove_html(series_):\n",
    "    return series_.map(lambda x: re.sub(\"[^A-Za-z0-9 ]\\w+;\", '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary punctuations\n",
    "(remove ones except !?$[], we will remove these after counting number of exclamation points and question marks, and if they occur by themselves (not as hashtag or emoji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary punctuations \n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "table_ = str.maketrans('', '', '!?#[]')\n",
    "punctuations = punctuations.translate(table_) + '�'\n",
    "\n",
    "def remove_punctuations(str_, punctuations):\n",
    "    table_ = str.maketrans('', '', punctuations)\n",
    "    return str_.translate(table_)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "Define stopwords. We don't want overall distribution of customer satisfaction of each company to affect our analysis so I'll remove some of the specific words for popular brands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "stopwords += ['apple', 'android', 'ipad', 'iphone', 'sxsw', '#sxsw', 'intel', \n",
    "             'atari', 'cisco', 'google', 'genentech', 'mac', 'pc']\n",
    "\n",
    "stopwords += string.punctuation\n",
    "stopwords += string.digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now tokenize and remove stopwords\n",
    "def tokenize(str_, stopwords):\n",
    "    str_ = str_.lower()\n",
    "    return [x for x in word_tokenize(str_) if x not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_text(df0, punctuations, stopwords):\n",
    "    df = df0.copy()\n",
    "    df['tweet'] = df.tweet.apply(replace_links)\n",
    "    df['tweet'] = remove_html(df.tweet)\n",
    "    df['tweet'] = df.tweet.apply(lambda x: remove_punctuations(x, punctuations))    \n",
    "    df['mention_count'] = df.tweet.apply(lambda x: count_exp(x, '@\\w+'))\n",
    "    df['link_count'] = df.tweet.apply(lambda x: count_exp(x, '{link}')) \n",
    "    df['tag_count'] = df.tweet.apply(lambda x: count_exp(x, '#\\w+'))\n",
    "    df['exclam_count'] = df.tweet.apply(lambda x: count_exp(x, '!'))\n",
    "    df['quest_count'] = df.tweet.apply(lambda x: count_exp(x, '\\?'))\n",
    "    df['tweet'] = df.tweet.apply(lambda x: tokenize(x, stopwords))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pp = prepro_text(X_train, punctuations, stopwords)\n",
    "X_val_pp = prepro_text(X_val, punctuations, stopwords)\n",
    "X_test_pp = prepro_text(X_test, punctuations, stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Pickles\n",
    "Export preprocessed data out as pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.mkdir('PKL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pp.to_pickle('PKL/X_train_pp.pkl')\n",
    "X_val_pp.to_pickle('PKL/X_val_pp.pkl')\n",
    "X_test_pp.to_pickle('PKL/X_test_pp.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
