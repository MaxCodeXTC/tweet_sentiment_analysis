{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"023.Model_BERT.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"GNZv0_f02UHc"},"source":["# BERT\n","\n","In this notebook, I will use Google's BERT (Bidrectional Encoder Representations from Transformers), which uses the attention. I'll try use PyTorch instead of Tensorflow."]},{"cell_type":"code","metadata":{"id":"aX7dXqmV3s_Y"},"source":["# mounting google drive\n","from google.colab import drive\n","from os.path import join\n","ROOT = '/content/drive'\n","drive.mount(ROOT)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UcjtpPf732iN"},"source":["cd 'drive/My Drive/Colab Notebooks/tweet_sentiment_analysis'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jUVXkTTaCUWe"},"source":["#pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J-nuvg1VCcPu"},"source":["#pip install pandas -U"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HKLwTDIA2UHe"},"source":["import pandas as pd\n","import numpy as np\n","\n","import torch\n","from torch.utils.data import *\n","from torch.nn.utils import clip_grad_norm_\n","\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from transformers import get_linear_schedule_with_warmup"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZHnvBIm92wl7"},"source":["# check GPU\n","torch.cuda.is_available()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"slRwDMOY2UHi"},"source":["X_train = pd.read_pickle('PKL/X_train_fin.pkl')\n","X_val = pd.read_pickle('PKL/X_val_fin.pkl')\n","y_train = pd.read_csv('DATA/y_train.csv', index_col=0)\n","y_val = pd.read_csv('DATA/y_val.csv', index_col=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tWXP1QPn2UHm"},"source":["X_train = X_train['tweet']\n","X_val = X_val['tweet']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5wzh6eau2UHp"},"source":["y_train = y_train['sentiment']\n","y_val = y_val['sentiment']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T_5vKR2u2UHw"},"source":["## Preprocess\n","Turning texts into tokens"]},{"cell_type":"code","metadata":{"id":"0vO6Jkx62UHx"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', from_pt = True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BUzqx2Fs2UH2"},"source":["add paddings to ids"]},{"cell_type":"code","metadata":{"id":"Dh0amvqZ2UH3"},"source":["X_tr_ids = tokenizer.batch_encode_plus(X_train, padding = True,\n","                                       return_token_type_ids=False,\n","                                      return_tensors = 'pt', \n","                                      max_length = 30, \n","                                      pad_to_max_length = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XDBa_rwz2UH6"},"source":["X_val_ids = tokenizer.batch_encode_plus(X_val, padding = True,\n","                                       return_token_type_ids=False,\n","                                      return_tensors = 'pt', \n","                                      max_length = 30, \n","                                      pad_to_max_length = True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KjDbnCxe2UH-"},"source":["Converting y_values"]},{"cell_type":"code","metadata":{"id":"oTNLcRzi2UH_"},"source":["classes_ind = dict(zip(set(y_train), range(3)))\n","y_train = torch.tensor([classes_ind[y] for y in y_train])\n","y_val = torch.tensor([classes_ind[y] for y in y_val])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0OlcrnlX2UIB"},"source":["### Creating the tensor datasets for PyTorch\n","Not we have the tensors. Let's create the dataloaders."]},{"cell_type":"code","metadata":{"id":"_Cskk31Y2UIC"},"source":["X_train_set = TensorDataset(X_tr_ids['input_ids'], X_tr_ids['attention_mask'], y_train)\n","tr_dataloader = DataLoader(X_train_set, sampler = RandomSampler(X_train_set), \n","                          batch_size = 32)\n","\n","\n","X_val_set = TensorDataset(X_val_ids['input_ids'], X_val_ids['attention_mask'], y_val)\n","val_dataloader = DataLoader(X_val_set, sampler = SequentialSampler(X_val_set), \n","                          batch_size = 32)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jgOQhNtvKWon"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0b8_xDhG2UIE"},"source":["### Modeling\n","Now time to fine tune"]},{"cell_type":"code","metadata":{"id":"ncql4INf2UIF"},"source":["model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n","                                                      num_labels = 3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"soyVSHs88Mjy"},"source":["Below code is an adaptation of these two sources (https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03, https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py)\n"]},{"cell_type":"code","metadata":{"id":"p6IkR4cn55Pc"},"source":["model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p0REP-vnfuRo","executionInfo":{"status":"ok","timestamp":1601160445332,"user_tz":240,"elapsed":4099,"user":{"displayName":"Eunjoo Byeon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNDgs1peryXK4HkjEYhAvrR91SVrK5RPgShjsl=s64","userId":"13849574159958514030"}}},"source":["param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'gamma', 'beta']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.0}\n","]"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q0KCkCpQ2UII"},"source":["# parameters\n","optimizer = AdamW(optimizer_grouped_parameters, lr = 5e-5)\n","epochs = 2\n","steps = len(tr_dataloader) * epochs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"evXX7aciAPg9"},"source":["from sklearn.metrics import f1_score, balanced_accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RWfODjtT-7ev"},"source":["# macro f1 score\n","def macro_f1 (preds, labels):\n","  pred_f = np.argmax(preds, axis = 1).flatten()\n","  labels_f = labels.flatten()\n","  return f1_score(labels_f, pred_f, average = 'macro')\n","\n","def avg_accuracy (preds, labels):\n","  pred_f = np.argmax(preds, axis = 1).flatten()\n","  labels_f = labels.flatten()\n","  return balanced_accuracy_score(labels_f, pred_f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-zIo4j-UDAVc"},"source":["device = torch.device('cuda')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E8Sfv5tQ2UIL"},"source":["# copying to GPU\n","def copy_GPU(data):\n","  input_ids = data[0].to(device)\n","  input_mask = data[1].to(device)\n","  labels = data[2].to(device)\n","  return input_ids, input_mask, labels\n","\n","history = [] \n","# training\n","for epoch in range(epochs):\n","  total_loss = 0.0\n","  model.train()\n","  for data in tr_dataloader:\n","    # reset gradient\n","    optimizer.zero_grad()\n","\n","    input_ids, input_mask, labels = copy_GPU(data)\n","\n","    # forward pass\n","    loss, logits = model(input_ids, attention_mask = input_mask, labels=labels)\n","\n","    total_loss += loss.item()\n","\n","    # backward pass\n","    loss.backward()\n","    \n","    #clip_grad_norm_(model.parameters(), 1.0)\n","\n","    # update step\n","    optimizer.step()\n","\n","  avg_tr_loss = total_loss / len(tr_dataloader)            \n","  print(f\"Epoch {epoch+1} -- avg training loss: {round(avg_tr_loss, 4)}\")\n","  \n","  #evaluation\n","  model.eval()\n","  total_val_loss = 0.0\n","  total_f1 = 0.0\n","  total_acc = 0.0\n","  for data in val_dataloader:\n","    input_ids, input_mask, labels = copy_GPU(data)\n","\n","    with torch.no_grad():\n","     loss, logits = model(input_ids, attention_mask = input_mask, \n","                          labels=labels)\n","\n","     logits = logits.detach().cpu().numpy()\n","     label_ids = labels.to('cpu').numpy()\n","     \n","     total_val_loss += loss.item()\n","     total_f1 += macro_f1(logits, label_ids)\n","     total_acc += avg_accuracy(logits, label_ids)\n","  \n","  avg_val_f1 = total_f1 / len(val_dataloader)\n","  avg_val_acc = total_acc / len(val_dataloader)\n","  avg_val_loss = total_val_loss / len(val_dataloader)\n","  print(f\"  avg validation loss: {round(avg_val_loss, 4)}\")\n","  print(f\"  avg validation accuracy: {round(avg_val_acc, 4)}\")\n","  print(f\"  avg validation f1 score: {round(avg_val_f1, 4)}\")\n","\n","  history.append({'epoch': epoch + 1, \n","                  'training loss': avg_tr_loss, \n","                  'validation loss': avg_val_loss, \n","                  'validation accuracy': avg_val_acc,\n","                  'validation F1': avg_val_f1\n","                  })\n","print('training completed')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"myj6riViGbIO"},"source":["results = pd.DataFrame(history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w88M2OGQC69J"},"source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","# Plot the learning curve.\n","plt.plot(results['training loss'], label=\"Train\")\n","plt.plot(results['validation loss'], label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([0, 1, 2, 3], [1, 2, 3, 4])\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YCS-J9aSGltd"},"source":["# validation predictions\n","y_pred = []\n","y_test0 = []\n","for data in val_dataloader:\n","    input_ids, input_mask, labels = copy_GPU(data)\n","\n","    with torch.no_grad():\n","     loss, logits = model(input_ids, attention_mask = input_mask, \n","                          labels=labels)\n","\n","     logits = logits.detach().cpu().numpy()\n","     label_ids = labels.to('cpu').numpy()\n","     \n","     y_pred.extend(logits)\n","     y_test0.extend(label_ids)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2sL9fbyJnq-7","executionInfo":{"status":"ok","timestamp":1601161339039,"user_tz":240,"elapsed":1239,"user":{"displayName":"Eunjoo Byeon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNDgs1peryXK4HkjEYhAvrR91SVrK5RPgShjsl=s64","userId":"13849574159958514030"}}},"source":["from sklearn.metrics import accuracy_score, cohen_kappa_score, classification_report"],"execution_count":88,"outputs":[]},{"cell_type":"code","metadata":{"id":"hLM6vJgDpIwj","executionInfo":{"status":"ok","timestamp":1601161340501,"user_tz":240,"elapsed":1263,"user":{"displayName":"Eunjoo Byeon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNDgs1peryXK4HkjEYhAvrR91SVrK5RPgShjsl=s64","userId":"13849574159958514030"}}},"source":["inv_classes = {v:k for k, v in classes_ind.items()}"],"execution_count":89,"outputs":[]},{"cell_type":"code","metadata":{"id":"vwfnF9GrnPYm","executionInfo":{"status":"ok","timestamp":1601161341753,"user_tz":240,"elapsed":2219,"user":{"displayName":"Eunjoo Byeon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNDgs1peryXK4HkjEYhAvrR91SVrK5RPgShjsl=s64","userId":"13849574159958514030"}}},"source":["def change_to_classes(target):\n","    tmp = np.argmax(target, axis = 1)\n","    return [inv_classes[x] for x in tmp]\n","\n","def evaluate(y_pred, target):\n","    y_val = [inv_classes[x] for x in target]\n","    y_pred = change_to_classes(y_pred)\n","    print('Accuracy', round(accuracy_score(y_val, y_pred), 4))\n","    print('Cohens Kappa', round(cohen_kappa_score(y_val, y_pred), 4))\n","\n","    print(classification_report(y_val, y_pred))"],"execution_count":90,"outputs":[]},{"cell_type":"code","metadata":{"id":"n33fiiEdnuW9","executionInfo":{"status":"ok","timestamp":1601161341754,"user_tz":240,"elapsed":1439,"user":{"displayName":"Eunjoo Byeon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNDgs1peryXK4HkjEYhAvrR91SVrK5RPgShjsl=s64","userId":"13849574159958514030"}},"outputId":"bc7de7ad-e12d-48de-bbb2-9645ad774397","colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["evaluate(y_pred, y_test0)"],"execution_count":91,"outputs":[{"output_type":"stream","text":["Accuracy 0.6711\n","Cohens Kappa 0.3069\n","                                    precision    recall  f1-score   support\n","\n","                  Negative emotion       0.44      0.10      0.16        71\n","No emotion toward brand or product       0.69      0.86      0.76       666\n","                  Positive emotion       0.63      0.45      0.52       385\n","\n","                          accuracy                           0.67      1122\n","                         macro avg       0.59      0.47      0.48      1122\n","                      weighted avg       0.65      0.67      0.64      1122\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fxGs1okInxFj","executionInfo":{"status":"ok","timestamp":1601161021878,"user_tz":240,"elapsed":1458,"user":{"displayName":"Eunjoo Byeon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNDgs1peryXK4HkjEYhAvrR91SVrK5RPgShjsl=s64","userId":"13849574159958514030"}},"outputId":"39574621-4f07-4aa4-e9af-881198505da2","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["y_val"],"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([2, 1, 1,  ..., 2, 1, 1])"]},"metadata":{"tags":[]},"execution_count":64}]}]}